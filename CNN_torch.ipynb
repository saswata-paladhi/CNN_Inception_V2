{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path= r'C:\\Workstation\\Chest_Cancer\\train'\n",
    "validation_path= r'C:\\Workstation\\Chest_Cancer\\valid'\n",
    "test_path= r'C:\\Workstation\\Chest_Cancer\\test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "resize_tuple= (64,64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(429, 353)\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "img= Image.open(r'C:\\Workstation\\Chest_Cancer\\test\\squamous.cell.carcinoma\\000117 (3).png')\n",
    "print(img.size)\n",
    "img= transforms.functional.resize(img, resize_tuple)\n",
    "img= transforms.functional.adjust_sharpness(img, 4)\n",
    "img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform= transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(resize_tuple),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.RandomRotation(15),\n",
    "        transforms.RandomAutocontrast(p=0.7),\n",
    "        transforms.RandomAdjustSharpness(8, p=0.8),\n",
    "        transforms.ToTensor()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_transform= transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(resize_tuple),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.RandomRotation(15),\n",
    "        transforms.RandomAutocontrast(p=0.7),\n",
    "        transforms.RandomAdjustSharpness(8, p=0.8),\n",
    "        transforms.ToTensor()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transform= transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(resize_tuple),\n",
    "        transforms.ToTensor()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset= torchvision.datasets.ImageFolder(train_path, transform= train_transform)\n",
    "validation_dataset= torchvision.datasets.ImageFolder(validation_path, transform= validation_transform)\n",
    "test_dataset= torchvision.datasets.ImageFolder(test_path, transform= test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_load= DataLoader(train_dataset, batch_size= 64, shuffle= True)\n",
    "validation_load= DataLoader(validation_dataset, batch_size= 64, shuffle= True)\n",
    "test_load= DataLoader(test_dataset, batch_size= 64, shuffle= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing a simple CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.conv1= nn.Conv2d(3,32,2,stride=2)\n",
    "        self.pool1= nn.MaxPool2d(4,2)\n",
    "        self.conv2= nn.Conv2d(32,64,5)\n",
    "        self.pool2= nn.AvgPool2d(3,1)\n",
    "        self.fc1= nn.Linear(9*9*64, 64, bias= True)\n",
    "        self.fc2= nn.Linear(64,24, bias=True)\n",
    "        self.fc3= nn.Linear(24,4)\n",
    "        self.relu= nn.ReLU()\n",
    "        self.out= nn.Softmax(dim=1)\n",
    "    def forward(self, image):\n",
    "        out= self.relu(self.conv1(image))\n",
    "        out= self.relu(self.pool1(out))\n",
    "        out= self.relu(self.conv2(out))\n",
    "        out= self.relu(self.pool2(out))\n",
    "        out= torch.flatten(out,1)\n",
    "        out= self.relu(self.fc1(out))\n",
    "        out= self.relu(self.fc2(out))\n",
    "        out= self.out(self.fc3(out))\n",
    "        return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing an inception v2 here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_inception_v2(nn.Module):                                      \n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.conv= nn.Conv2d(3, 64, 5, stride= 1)          #In_channel is 3 cuz its an RGB image. out_channel signifies number of filters used. In this case its 10\n",
    "        self.pool= nn.MaxPool2d(4,2)\n",
    "        self.conv1dim= nn.Conv2d(64, 32, 1)\n",
    "        self.conv3dim= nn.Conv2d(32, 128, 3, padding= 1)\n",
    "        self.conv5dim= nn.Conv2d(32, 64, 5, padding= 2) \n",
    "        self.poolincep= nn.MaxPool2d(3,1,padding= 1) \n",
    "        self.convpin= nn.Conv2d(256,256, 3, 2)\n",
    "        self.fc1= nn.Linear(14*14*256, 256)      \n",
    "        self.drop1= nn.Dropout(p= 0.2)\n",
    "        self.fc2= nn.Linear(256,64)\n",
    "        self.drop2= nn.Dropout(p= 0.1)\n",
    "        self.fc3= nn.Linear(64, 16)\n",
    "        self.drop3= nn.Dropout(p= 0.1)\n",
    "        self.fc4= nn.Linear(16,4, bias=True)\n",
    "        self.relu= nn.ReLU()\n",
    "        self.sig= nn.Sigmoid()\n",
    "        self.soft= nn.Softmax(dim=1)\n",
    "    def forward(self, image):\n",
    "        out= self.relu(self.conv(image))\n",
    "        out= self.relu(self.pool(out))\n",
    "        out1= self.conv1dim(out)\n",
    "        out3= self.conv3dim(out1)\n",
    "        out5= self.conv5dim(out1)\n",
    "        outpool= self.conv1dim(self.poolincep(out))\n",
    "        #print(f'out3 {out3.shape}, out5 {out5.shape}, outpool {outpool.shape} ')\n",
    "        out= torch.cat((out1,out3,out5,outpool), dim=1)             #We use dim=1 for channel wise concat\n",
    "        #print(out.shape)\n",
    "        out= self.relu(self.convpin(out))\n",
    "        out= torch.flatten(out,1)                                   #start dim is 1 cuz dim 0 is the batch size\n",
    "        out= self.relu(self.fc1(out))\n",
    "        out= self.drop1(out)\n",
    "        out= self.relu(self.fc2(out))\n",
    "        out= self.drop2(out)\n",
    "        out= self.relu(self.fc3(out))\n",
    "        out= self.soft(self.fc4(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "device= torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_in= CNN_inception_v2()\n",
    "model_in.to(device)\n",
    "criterion= nn.CrossEntropyLoss()\n",
    "optimizer_incep= optim.Adam(model_in.parameters(), lr= 0.0005, weight_decay= 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn= CNN()\n",
    "model_cnn.to(device)\n",
    "criterion= nn.CrossEntropyLoss()\n",
    "optimizer_cnn= optim.Adam(model_cnn.parameters(), lr= 0.001, weight_decay= 0.01) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_load, epochs, optimizer):\n",
    "    for i in range(epochs):\n",
    "        running_loss=0\n",
    "        for image, labels in train_load:\n",
    "            optimizer.zero_grad()\n",
    "            #label_enc= F.one_hot(labels)\n",
    "            pred= model(image.to(device))\n",
    "            loss= criterion(pred, labels.to(device))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss+=loss.item()\n",
    "        print(running_loss/len(train_load))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3804609656333924\n",
      "1.371563971042633\n",
      "1.371424102783203\n",
      "1.372449231147766\n",
      "1.3730290532112122\n",
      "1.3737617492675782\n",
      "1.375681495666504\n",
      "1.3689804077148438\n",
      "1.3652426958084107\n",
      "1.3646875262260436\n",
      "1.3624593257904052\n",
      "1.33995840549469\n",
      "1.3054252028465272\n",
      "1.3378654837608337\n",
      "1.3170722842216491\n",
      "1.3040712833404542\n",
      "1.2911670207977295\n",
      "1.2834850430488587\n",
      "1.2779454946517945\n",
      "1.2855224370956422\n",
      "1.2810354709625245\n",
      "1.274007225036621\n",
      "1.267010235786438\n",
      "1.2772723078727721\n",
      "1.2609099864959716\n",
      "1.2538387775421143\n",
      "1.2668240070343018\n",
      "1.2548731446266175\n",
      "1.2558371782302857\n",
      "1.2557998299598694\n",
      "1.2664166927337646\n",
      "1.2554449796676637\n",
      "1.2593890309333802\n",
      "1.2423535466194153\n",
      "1.2454298615455628\n",
      "1.241891360282898\n",
      "1.2555890321731566\n",
      "1.2479472875595092\n",
      "1.2480343222618102\n",
      "1.2453110694885254\n",
      "1.2344878673553468\n",
      "1.2413658618927002\n",
      "1.22824684381485\n",
      "1.2233487606048583\n",
      "1.2248245120048522\n",
      "1.237891936302185\n",
      "1.2225700974464417\n",
      "1.2196126222610473\n",
      "1.2038466453552246\n",
      "1.2204222202301025\n",
      "1.2263949155807494\n",
      "1.214055562019348\n",
      "1.2112244844436646\n",
      "1.1999340176582336\n",
      "1.2211639165878296\n",
      "1.21909818649292\n",
      "1.187269914150238\n",
      "1.2603606581687927\n",
      "1.2629163026809693\n",
      "1.235666286945343\n",
      "1.2252116322517395\n",
      "1.2048086285591126\n",
      "1.181730318069458\n",
      "1.1954047083854675\n",
      "1.2057374238967895\n",
      "1.1868525385856628\n",
      "1.1934066772460938\n",
      "1.1967299699783325\n",
      "1.1702991843223571\n",
      "1.161457085609436\n",
      "1.1697401523590087\n",
      "1.1905425429344176\n",
      "1.165308380126953\n",
      "1.1476250052452088\n",
      "1.1612394332885743\n",
      "1.1636012673377991\n",
      "1.154873263835907\n",
      "1.1623451113700867\n",
      "1.1404006600379943\n",
      "1.1650748610496522\n"
     ]
    }
   ],
   "source": [
    "model_in= train_model(model_in,train_load,epochs=80, optimizer= optimizer_incep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score, recall_score, precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN_inception_v2(\n",
       "  (conv): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=4, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv1dim): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (conv3dim): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv5dim): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (poolincep): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "  (convpin): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))\n",
       "  (fc1): Linear(in_features=50176, out_features=256, bias=True)\n",
       "  (drop1): Dropout(p=0.2, inplace=False)\n",
       "  (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
       "  (drop2): Dropout(p=0.1, inplace=False)\n",
       "  (fc3): Linear(in_features=64, out_features=16, bias=True)\n",
       "  (drop3): Dropout(p=0.1, inplace=False)\n",
       "  (fc4): Linear(in_features=16, out_features=4, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (sig): Sigmoid()\n",
       "  (soft): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_in.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_test(data_load, model):\n",
    "    for image, label in data_load:\n",
    "        pred= model(image)\n",
    "        label= F.one_hot(label)\n",
    "        print(accuracy_score(np.argmax(pred.detach().numpy(),axis=1), np.argmax(label.detach().numpy(), axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6875\n",
      "0.640625\n",
      "0.546875\n",
      "0.515625\n",
      "0.59375\n",
      "0.609375\n",
      "0.765625\n",
      "0.5625\n",
      "0.5625\n",
      "0.5675675675675675\n"
     ]
    }
   ],
   "source": [
    "accuracy_test(train_load, model_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.53125\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "accuracy_test(validation_load, model_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.015625\n",
      "0.03125\n",
      "0.203125\n",
      "0.640625\n",
      "0.5254237288135594\n"
     ]
    }
   ],
   "source": [
    "accuracy_test(test_load, model_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model_in\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3805128574371337\n",
      "1.3739952087402343\n",
      "1.3691155791282654\n",
      "1.370361065864563\n",
      "1.372713530063629\n",
      "1.3657234072685243\n",
      "1.3554505944252013\n",
      "1.3682892799377442\n",
      "1.3565939903259276\n",
      "1.3519548416137694\n",
      "1.3281307101249695\n",
      "1.3167996048927306\n",
      "1.3041876673698425\n",
      "1.2800137639045714\n",
      "1.2771039366722108\n",
      "1.274445855617523\n",
      "1.2732572913169862\n",
      "1.2746810078620912\n",
      "1.2692505359649657\n",
      "1.2620531797409058\n",
      "1.2671003818511963\n",
      "1.27033371925354\n",
      "1.2549307227134705\n",
      "1.2549286961555481\n",
      "1.2463118433952332\n",
      "1.2586859583854675\n",
      "1.2499377131462097\n",
      "1.2493787288665772\n",
      "1.2502614498138427\n",
      "1.2508395433425903\n",
      "1.2476540446281432\n",
      "1.2444623827934265\n",
      "1.2403372883796693\n",
      "1.2224214315414428\n",
      "1.2309369087219237\n",
      "1.2264047384262085\n",
      "1.2235989212989806\n",
      "1.2203340530395508\n",
      "1.2249328017234802\n",
      "1.2398862838745117\n",
      "1.2198015570640564\n",
      "1.2152350068092346\n",
      "1.2066137790679932\n",
      "1.2438100099563598\n",
      "1.2513516902923585\n",
      "1.225580060482025\n",
      "1.209929347038269\n",
      "1.2102306604385376\n",
      "1.2381454706192017\n",
      "1.2359827280044555\n"
     ]
    }
   ],
   "source": [
    "model_cnn= train_model(model_cnn, train_load, epochs= 50,optimizer= optimizer_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (conv1): Conv2d(3, 32, kernel_size=(2, 2), stride=(2, 2))\n",
       "  (pool1): MaxPool2d(kernel_size=4, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (pool2): AvgPool2d(kernel_size=3, stride=1, padding=0)\n",
       "  (fc1): Linear(in_features=5184, out_features=64, bias=True)\n",
       "  (fc2): Linear(in_features=64, out_features=24, bias=True)\n",
       "  (fc3): Linear(in_features=24, out_features=4, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (out): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_cnn.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.453125\n",
      "0.53125\n",
      "0.5\n",
      "0.484375\n",
      "0.5\n",
      "0.59375\n",
      "0.578125\n",
      "0.453125\n",
      "0.359375\n",
      "0.5945945945945946\n"
     ]
    }
   ],
   "source": [
    "accuracy_test(train_load, model_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.40625\n",
      "0.375\n"
     ]
    }
   ],
   "source": [
    "accuracy_test(validation_load, model_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.828125\n",
      "0.609375\n",
      "0.203125\n",
      "0.59375\n",
      "0.23728813559322035\n"
     ]
    }
   ],
   "source": [
    "accuracy_test(test_load, model_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model_cnn\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 (tags/v3.10.9:1dd9be6, Dec  6 2022, 20:01:21) [MSC v.1934 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2306d57543b858c86fdb80a356bd8c0aaaab8cdcc85977051984cdf50a86daab"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
